{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch3_3_2_random_signoid_bias.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPa1MDsW5nKp8gWKH2C/gup"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3sY4zY8bgrM8","colab_type":"text"},"source":["---\n","title: \"Tensorflow 2.0 Tutorial ch3.2.2 - 난수 생성 및 시그모이드 함수 편향성\"\n","date: 2020-04-05T11:20:30+09:00\n","tags:\n","  - \"Deep Learning\"\n","  - \"Python\"\n","  - \"Google Colab\"\n","  - \"Tensorflow 2.0\"\n","  - \"시그노이드\"\n","  - \"Random Signoid\"\n","  - \"Random Signoid Bias\"\n","  - \"텐서플로 2.0\"\n","  - \"텐서플로 2.0 튜토리얼\"\n","  - \"Tensorflow 2.0 Tutorial\"\n","categories:\n","  - \"Deep Learning\"\n","  - \"딥러닝\"\n","  - \"텐서플로 2.0\"\n","  - \"Python\"\n","  - \"Tensorflow 2.0\"\n","  - \"텐서플로 2.0 튜토리얼\"\n","  - \"Tensorflow 2.0 Tutorial\"\n","menu: \n","  python:\n","    name: Tensorflow 2.0 Tutorial ch3.2 - 난수 생성 및 시그모이드 함수\n","---\n","\n","## 공지\n","\n","- 본 Tutorial은 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n","- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다. \n","- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n","\n","## Tutorial\n","\n","이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n","\n","- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n","- [Tensorflow 2.0 Tutorial ch3.2 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_2_random_signoid/)\n","\n","## I. 편향성 (Bias)\n","\n","지난 시간에 경사하강법 원리를 통해 오차가 적어지는 것을 확인할 수 있었다. \n","\n","$w = w + x \\times a \\times error$\n","\n","여기에서 입력으로 0을 넣게 되면 출력으로 1을 얻는 뉴런은 어떻게 만들 수 있을까? 지난시간에 배운 내용으로 확인해보자. "]},{"cell_type":"code","metadata":{"id":"_mLI7M5unSqW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598235342422,"user_tz":-540,"elapsed":2460,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"324af434-5435-4111-cbac-09ab9c3b67ec"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5_Q1nj4io7x3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598235381663,"user_tz":-540,"elapsed":651,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}}},"source":["import math\n","def sigmoid(x): \n","  return 1 / (1 + math.exp(-x))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgsJMMi1o4NX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1598235388861,"user_tz":-540,"elapsed":724,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"d8313d38-b3cc-4560-9818-168ccc2d9804"},"source":["x = 0\n","y = 1\n","w = tf.random.normal([1], 0, 1)\n","\n","for i in range(1000): \n","  output = sigmoid(x * w)\n","  error = y - output\n","  w = w + x * 0.1 * error\n","\n","  if i % 100 == 99:\n","    print(i, error, output)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["99 0.5 0.5\n","199 0.5 0.5\n","299 0.5 0.5\n","399 0.5 0.5\n","499 0.5 0.5\n","599 0.5 0.5\n","699 0.5 0.5\n","799 0.5 0.5\n","899 0.5 0.5\n","999 0.5 0.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C-SSCMdgpTYZ","colab_type":"text"},"source":["경사 하강법의 원리를 적용했지만, `error`가 변하지 않았고, 출력도 변하지 않았다. 수식을 입력하면, `x = 0` 이므로, `w`에 더해지는 값은 없다. `1,000`번의 실행 동안 `w`값은 변하지 않았다. \n","\n","> 여기에서, 책은 간단히 짚고 넘어갔지만, 왜 딥러닝에서 수학적인 원리가 중요한지 알 수 있다. 일반적으로 개발자 분들은 이러한 부분들을 크게 생각 안하는 경우가 많지만, 기능을 구현하는 것 보다 더 중요한 건 코드를 짤 때, 수학적인 원리도 같이 고민해야 하는 경우가 많다. 머신러닝도 그렇지만, 딥러닝도 다양한 인자들이 존재하고, 이를 이해하려면 공식문서를 잘 참고해야 하며, 더 좋은 성과 및 퍼포먼스를 내려면 결국엔 관련 Thesis, 즉 논문을 읽어낼줄 알아야 한다. 그래야 성과가 나온다. 딥러닝은 1년 단위로 논문이 나오는 걸 잊지 말자! \n","\n","그럼 어떻게 해결해야 할까? 이러한 경우를 방지하기 위해 `bias` 라는 개념이 존재한다. 간단히 설명하면 x가 0이 들어오면 대안으로 1이 들어온 것처럼 코드를 작성하는 것이다. \n","\n","수식에서는 관용적으로 `bias`의 앞 글자인 `b`를 쓴다. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"aGe8CnwerksE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1598235478368,"user_tz":-540,"elapsed":610,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"afe29ebf-0f8b-40ea-871d-69c4f1a87077"},"source":["x = 0\n","y = 1\n","w = tf.random.normal([1], 0, 1)\n","b = tf.random.normal([1], 0, 1)\n","\n","for i in range(1000): \n","  output = sigmoid(x * w + 1 * b)\n","  error = y - output\n","  w = w + x * 0.1 * error\n","  b = b + 1 * 0.1 * error\n","\n","  if i % 100 == 99:\n","    print(i, error, output)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["99 0.11634698253168496 0.883653017468315\n","199 0.05606773179743474 0.9439322682025653\n","299 0.03648716917148831 0.9635128308285117\n","399 0.02694753477629508 0.9730524652237049\n","499 0.021330455607198706 0.9786695443928013\n","599 0.017637629486535578 0.9823623705134644\n","699 0.015028046992540678 0.9849719530074593\n","799 0.013087489574964306 0.9869125104250357\n","899 0.011588586840112325 0.9884114131598877\n","999 0.010396346887092789 0.9896036531129072\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FxuChwP8r_xk","colab_type":"text"},"source":["편향을 나타내는 `b`가 추가되었고, `w`처럼 초기화를 진행한다. 그리고, `sigmoid()` 안에 각 입력에 가중치와 편향을 곱해서 더해준 뒤 시그모이드 함수를 취한다. 기대출력과 실제출력의 차이인 `error`로 `w`와 `b`를 각각 업데이트해서 뉴런을 학습시킨다. \n","\n","프로그램을 실행한 결과, `error`는 0에 가까워지고, `output`은 기대출력인 1에 가까워진다. \n","\n","이번 시간까지는 간단하게 워밍업을 진행하였고, 다음 시간부터는 순차적으로 신경망 네트워크의 기본 원리에 대해 학습하도록 한다. \n","\n","## II. Reference\n","\n","김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스."]}]}