{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch7_1_2_RNN_theory(1).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNIaZmOPiyDkPQQTtYrPK5w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"w_qDfvmcn_Za","colab_type":"text"},"source":["---\n","title: \"Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)\"\n","date: 2020-04-22T15:08:30+09:00\n","tags:\n","  - \"Deep Learning\"\n","  - \"Python\"\n","  - \"Google Colab\"\n","  - \"Tensorflow 2.0\"\n","  - \"Binary Classification\"\n","  - \"Classification\"\n","  - \"순환 신경망\"\n","  - \"Recurrent Neural Network\"\n","  - \"RNN\"\n","  - \"SimpleRNN\"\n","  - \"LSTM\"\n","  - \"GRU\"\n","  - \"텐서플로 2.0\"\n","  - \"텐서플로 2.0 튜토리얼\"\n","  - \"Image Augmentation\"\n","  - \"Tensorflow 2.0 Tutorial\"\n","categories:\n","  - \"Deep Learning\"\n","  - \"딥러닝\"\n","  - \"텐서플로 2.0\"\n","  - \"Python\"\n","  - \"Tensorflow 2.0\"\n","  - \"텐서플로 2.0 튜토리얼\"\n","  - \"Tensorflow 2.0 Tutorial\"\n","menu: \n","  python:\n","    name: Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)\n","---\n","\n","## 공지\n","\n","- 본 Tutorial은 교재 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n","\n","- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다. \n","\n","![](/img/tensorflow2.0/book.jpg)<!-- -->\n","\n","\n","- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n","\n","\n","## Tutorial\n","\n","이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n","\n","- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n","- [Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)\n","- [Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성](https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/)\n","- [Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND](https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/)\n","- [Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/)\n","- [Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/)\n","- [Tensorflow 2.0 Tutorial ch4.1 - 선형회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/)\n","- [Tensorflow 2.0 Tutorial ch4.2 - 다항회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/)\n","- [Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/)\n","- [Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트](https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/)\n","- [Tensorflow 2.0 Tutorial ch5.1 - 분류](https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/)\n","- [Tensorflow 2.0 Tutorial ch5.2 - 다항분류](https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/)\n","- [Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST](https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/)\n","- [Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론](https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/)\n","- [Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습](https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/)\n","- [Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기](https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/)\n","\n","\n","## I. 개요\n","\n","순환 신경망(Recurrent Neural Network; RNN)은 지금까지 살펴본 네트워크와는 입력을 받아들이는 방식과 처리하는 방식에 약간 차이가 있습니다. 순환 신경망은 순서가 있는 데이터를 입력으로 받고, 같은 네트워크를 이용해 변화하는 입력에 대한 출력을 얻어냅니다. \n","\n","순서가 있는 데이터는 음악, 자연어, 날씨, 주가 등 시간의 흐름에 따라 변화하고 그 변화가 의미를 갖는 데이터입니다. \n","\n","## II. 순환 신경망의 구조\n","\n","우선 `CNN`과 `RNN`의 딥러닝 구조의 차이점에 대해 이미지[^1]로 확인하면 보다 직관적으로 이해가 될 수 있습니다. \n","\n","CNN의 구조는 본 교재를 계속 따라오셨다면 익숙하다시피, 아래와 같은 구조로 되어 있습니다. \n","\n","![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_01_CNN.png)\n","\n","그러나, RNN의 구조는 아래에서 확인할 수 있는 것처럼, 순환 모양의 화살표가 있다는 것이 차이점입니다. \n","\n","![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_01_RNN.png)\n","\n","순환 신경망의 특징에 대해 간단하게 요약하면 다음과 같습니다. \n","- 입력 X를 받아서, 출력 Y를 반환합니다.\n","- 순환구조를 가지고 있다; 어떤 레이어의 출력을 다시 입력으로 받는 구조를 말합니다. \n","- 순환 신경망은 입력과 출력의 길이에 제한이 없습니다. \n","- 순환 신경망은 이미지에 대한 설명을 생성하는 이미지 설명 생성, 문장의 긍정/부정을 판단하는 감성 분석, 하나의 언어를 다른 언어로 번역하는 기계 번역(Machine Translation) 등 다양한 용도로 활용됩니다. \n","\n","순환 신경망의 이론에 대한 자세한 설명은 교재 (`p. 174-5`)를 참조하시기를 바랍니다. \n","\n","## III. 주요 레이어 정리\n","순환 신경망의 가장 기초적인 레이어는 `SimpleRNN` 레이어이며, 이 레이어에서 출발한 `LSTM` 레이어 또는 `GRU`레이어가 주로 쓰입니다. 그리고, 자연어 처리를 위해서 꼭 알아둬야 하는 임베딩(`Embedding`)레이어도 같이 알아봅니다. \n","\n","### (1) SimpleRNN 레이어\n","`SimpleRNN`레이어는 가장 간단한 형태의 `RNN`레이업니다. 수식에 대한 설명은 교재(`p. 176`)를 참고합니다. 이 때 주로 사용되는 활성화 함수로는 `tanh`가 사용됩니다. `tanh`는 실수 입력을 받아 -1에서 1사이의 출력 값을 반환하는 활성하 함수이며, 이 활성화 함수 자리에 `ReLU`같은 다른 활성화함수를 쓸 수도 있습니다. \n","\n","`SimpleRNN` 레이어는 `tf.keras`에서 한 줄로 간단하게 생성이 가능합니다.\n","\n","```python\n","rnn1 = tf.keras.layers.SimpleRNN(units=1, activation='tanh', return_sequences=True)\n","```\n","\n","- `units`는 `SimpleRNN`의 레이어에 존재하는 뉴런의 수를 의미합니다. `return_sequences`는 출력으로 시퀀스 전체를 출력할지 여부를 나타내는 옵션이며, 여러 개의 `RNN 레이어`를 쌓을 때 쓰입니다. \n","\n","간단한 예제를 통해서 학습을 해봅니다. \n","\n","\n","[^1]: Different between CNN，RNN（Quote） Retrieved from https://medium.com/@Aj.Cheng/different-between-cnn-rnn-quote-7c224795db58\n"]},{"cell_type":"code","metadata":{"id":"w1gWw2ee1tMa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598347405753,"user_tz":-540,"elapsed":2622,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}}},"source":["# 텐서플로 2 버전 선택\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"lm4jB5GQzUDT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"status":"ok","timestamp":1598347419840,"user_tz":-540,"elapsed":920,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"5aab1053-55c4-4046-ddad-4933d55e9ed5"},"source":["X = []\n","Y = []\n","\n","for i in range(6):\n","  # [0, 1, 2, 3], [1, 2, 3, 4]\n","  lst = list(range(i,i+4))\n","\n","  # 위에서 구한 시퀀스의 숫자들을 각각 10으로 나눈 다음 저장합니다. \n","  # SimpleRNN에 각 타임스텝에 하나씩 숫자가 들어가기 때문에 여기서도 하나씩 분리해서 배열에 저장합니다. \n","  X.append(list(map(lambda c:[c/10], lst)))\n","\n","  # 정답에 해당하는 4, 5 등의 정수 역시 앞에서처럼 10으로 나눠서 저장합니다. \n","  Y.append((i+4)/10)\n","\n","X = np.array(X)\n","Y = np.array(Y)\n","\n","for i in range(len(X)): \n","  print(X[i], Y[i])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[0. ]\n"," [0.1]\n"," [0.2]\n"," [0.3]] 0.4\n","[[0.1]\n"," [0.2]\n"," [0.3]\n"," [0.4]] 0.5\n","[[0.2]\n"," [0.3]\n"," [0.4]\n"," [0.5]] 0.6\n","[[0.3]\n"," [0.4]\n"," [0.5]\n"," [0.6]] 0.7\n","[[0.4]\n"," [0.5]\n"," [0.6]\n"," [0.7]] 0.8\n","[[0.5]\n"," [0.6]\n"," [0.7]\n"," [0.8]] 0.9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k8ajfVcJ4o9i","colab_type":"text"},"source":["이제 `SimpleRNN` 레이어를 사용한 네트워크를 정의합니다. 모델 구조는 지금까지 계속 봐온 시퀀셜 모델이고, 출력을 위한 `Dense` 레이어가 뒤에 추가되어 있습니다. "]},{"cell_type":"code","metadata":{"id":"jU6FkwUD49JJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1598347428970,"user_tz":-540,"elapsed":6804,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"03abb8c3-a36d-49cb-b7b9-b6c34f9aadb9"},"source":["# 7.3 시퀀스 예측 모델 정의\n","model = tf.keras.Sequential([\n","    tf.keras.layers.SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","simple_rnn (SimpleRNN)       (None, 10)                120       \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 11        \n","=================================================================\n","Total params: 131\n","Trainable params: 131\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wakc-TmH5ZUV","colab_type":"text"},"source":["여기에서 주목해야 하는 코드는 `input_shape`입니다. 여기에서 `[4,1]`은 각각 `timesteps`, `input_dim`을 나타냅니다. 타입스텝은(timesteps)이란 순환 신경망이 입력에 대해 계산을 반복하는 횟수를 말하고, `input_dim`은 벡터의 크기를 나타냅니다. \n","\n","```\n","[[0. ]\n"," [0.1]\n"," [0.2]\n"," [0.3]] 0.4\n","```\n","\n","두번째의 4는 타임스텝, 세번째의 1은 `input_dim`이 됩니다. 그림을 참조하면 훨씬 이해하기 쉽습니다. (교재, p.180)\n","\n","시퀀스 예측 모델은 4 타임스텝에 걸쳐 입력을 받고, 마지막에 출력값을 다음 레이어로 반환합니다. 우리가 추가한 `Dense`레이어에는 별도의 활성화함수가 없기 때문에 $h_{3}$는 바로 $y_{3}$이 됩니다. 그리고 이 값과 0.4와의 차이가 `mse`, 즉 평균 제곱 오차(`Mean Squared Error`)가 됩니다. \n","\n","이제 훈련을 시킵니다. 이 때, `verbose`값을 0으로 놓으면 훈련 과정에서의 출력이 나오지 않습니다. "]},{"cell_type":"code","metadata":{"id":"WpFME3mM7tUf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"ok","timestamp":1598347436075,"user_tz":-540,"elapsed":4614,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"359e10a0-bea9-475e-9df4-ae5e7028c08a"},"source":["model.fit(X, Y, epochs=100, verbose=0)\n","print(model.predict(X))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[0.57561743]\n"," [0.6323522 ]\n"," [0.6787124 ]\n"," [0.7143893 ]\n"," [0.7397379 ]\n"," [0.7554892 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EqGJUwnj8kCW","colab_type":"text"},"source":["X가 주어졌을 때 학습된 모델이 시퀀스를 어떻게 예측하는지 확인해보면 얼추 비슷하게 예측하고 있음을 확인할 수 있습니다. 그렇다면 학습과정에서 본 적이 없는 테스트 데이터를 넣으면 어떨까요? `X`의 범위가 0.0~0.9 였으니, 양쪽으로 한 칸씩 더 나간 데이터를 입력합니다. \n"]},{"cell_type":"code","metadata":{"id":"xc1ADK7n83Ek","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1598347442955,"user_tz":-540,"elapsed":965,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"cfca3a91-5cf2-41ce-c2a9-58ce5186f52a"},"source":["print(model.predict(np.array([[[0.6], [0.7], [0.8], [0.9]]])))\n","print(model.predict(np.array([[[-0.1], [0.0], [0.1], [0.2]]])))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[[0.76253086]]\n","[[0.5097425]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4dMW7X6N9JpN","colab_type":"text"},"source":["1을 예측하기를 원한 데이터의 출력으로는 0.91을 0.3을 예측하기 원한 데이터의 출력으로는 0.22의 값을 반환했습니다. \n","\n","실무에서는 `SimpleRNN`보다는 `LSTM` 레이어와 `GRU`레이어를 사용합니다. \n","\n","### (2) LSTM 레이어\n","\n","`SimpleRNN` 레이어에는 한 가지 치명적인 단점이 존재합니다. 입력 데이터가 길어질수록, 즉 데이터의 타임스텝이 길어질수록 학습 능력이 떨어진다는 점입니다. 이를 장기의존성(Long-Term Dependency)문제라고 하며, 입력 데이터와 출력 사이의 길이가 멀어질수록 연관 관계가 적어집니다. \n","\n","![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_LongTermDependency.png)\n","\n","위 그림이 이러한 문제를 적절하게 표현한 것입니다. 입력 데이터가 길어지면 길어질수록 출력값의 연관 관계가 적어지는 것을 볼 수 있습니다. \n","\n","이러한 문제점을 해결하기 위해 `LSTM`이 제안 되었습니다.[^2] 셀로 나타낸 SimpleRNN과 LSTM의 계산 흐름을 보면 조금 이해가 될 것입니다. \n","\n","- 먼저 SimpleRNN의 그림은 아래와 같습니다. \n","\n","![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_SimpleRNN.png)\n","\n","여기에서는 타임스텝의 방향으로 $h_{t}$만 전달되고 있음을 확인할 수 있습니다. \n","\n","![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_LSTM.png)\n","\n","그런데, 여기에서는 셀 상테인 $c_{t}$가 평생선을 그리며 함께 전달되고 있습니다. 이처럼 타임스텝을 가로지르며 `LSTM` 셀 상태가 보존되기 때문에 장기의존성 문제를 해결할 수 있다는 것이 `LSTM`의 핵심 아이디어입니다. \n","\n","교재 184페이지를 보면 위 셀에 대한 수식이 존재합니다만, 수식에 대한 구체적인 이해가 자료가 필요하다면 크리스토퍼 올라(`Christopher Olah`)의 블로그 글을 참고합니다.[^3]\n","\n","`LSTM`의 학습 능력을 확인하기 위한 예제는 `LSTM`을 처음 제안한 논문에 나온 실험 여섯개 중 다섯 번째인 곱셈 문제(`Multiplication Problem`)입니다. 이 문제는 말 그대로 실수에 대해 곱셈을 하는 문제인데, 고려해야 할 실수의 범위가 100개이고 그 중에서 마킹된 두개의 숫자만 곱해야 한다는 특이한 문제입니다. \n","\n","[^2]: 1997년 셉 호흐라이터(Sepp Hochreiter) 유르겐 슈미트후버(Jurgen Schmidhuber)에 의해 제안됨, (S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997. https://www.bioinf.jku.at/publications/older/2604.pdf \n","\n","[^3]: Olah, Christopher. “Understanding LSTM Networks.” Understanding LSTM Networks -- Colah's Blog, colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n"]},{"cell_type":"code","metadata":{"id":"iFzH05KBbX9R","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598347449215,"user_tz":-540,"elapsed":1094,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}}},"source":["# 텐서플로 2 버전 선택\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","import numpy as np"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwswtlnMXDX_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598347460348,"user_tz":-540,"elapsed":1476,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"b83aa9e3-8db1-4127-a166-5f9cf81379d6"},"source":["X = []\n","Y = []\n","for i in range(3000): \n","  # 0 ~ 1 범위의 랜덤한 숫자 100개를 만듭니다. \n","  lst = np.random.rand(100)\n","\n","  # 마킹할 숫자 2개의 인덱스를 뽑습니다. \n","  idx = np.random.choice(100, 2, replace=False)\n","\n","  # 마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만듭니다. \n","  zeros=np.zeros(100)\n","  zeros[idx]=1\n","  \n","  # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X에 저장합니다. \n","  X.append(np.array(list(zip(zeros, lst))))\n","  # 마킹 인덱스가 1인 값만 서로 곱해서 Y에 저장합니다. \n","  Y.append(np.prod(lst[idx]))\n","\n","print(X[0], Y[0])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[0.         0.5124854 ]\n"," [0.         0.87336603]\n"," [0.         0.09271747]\n"," [0.         0.01008941]\n"," [0.         0.86350951]\n"," [1.         0.9701965 ]\n"," [0.         0.87996155]\n"," [0.         0.22073855]\n"," [0.         0.63944159]\n"," [0.         0.51789808]\n"," [0.         0.16698075]\n"," [0.         0.84297079]\n"," [0.         0.40266475]\n"," [0.         0.11087754]\n"," [0.         0.4696563 ]\n"," [0.         0.1058184 ]\n"," [0.         0.15458187]\n"," [0.         0.43218628]\n"," [0.         0.5303914 ]\n"," [1.         0.82088222]\n"," [0.         0.34472808]\n"," [0.         0.33135519]\n"," [0.         0.46459204]\n"," [0.         0.77059161]\n"," [0.         0.78790921]\n"," [0.         0.90223749]\n"," [0.         0.40477974]\n"," [0.         0.59190036]\n"," [0.         0.20670783]\n"," [0.         0.42473646]\n"," [0.         0.69360108]\n"," [0.         0.5214298 ]\n"," [0.         0.18933175]\n"," [0.         0.57914701]\n"," [0.         0.43724148]\n"," [0.         0.79008302]\n"," [0.         0.20265632]\n"," [0.         0.43699629]\n"," [0.         0.37147135]\n"," [0.         0.90866448]\n"," [0.         0.20148083]\n"," [0.         0.75596578]\n"," [0.         0.21754752]\n"," [0.         0.83031233]\n"," [0.         0.0850188 ]\n"," [0.         0.29044596]\n"," [0.         0.07992013]\n"," [0.         0.35789831]\n"," [0.         0.62031507]\n"," [0.         0.86916871]\n"," [0.         0.1613581 ]\n"," [0.         0.40995821]\n"," [0.         0.29071631]\n"," [0.         0.65575324]\n"," [0.         0.81475152]\n"," [0.         0.17342277]\n"," [0.         0.28019456]\n"," [0.         0.07432482]\n"," [0.         0.90948291]\n"," [0.         0.14759759]\n"," [0.         0.32055408]\n"," [0.         0.44790661]\n"," [0.         0.80552981]\n"," [0.         0.90282184]\n"," [0.         0.063746  ]\n"," [0.         0.95131742]\n"," [0.         0.9278657 ]\n"," [0.         0.19596275]\n"," [0.         0.55171566]\n"," [0.         0.11661367]\n"," [0.         0.7097312 ]\n"," [0.         0.28564645]\n"," [0.         0.7092399 ]\n"," [0.         0.77673293]\n"," [0.         0.91299808]\n"," [0.         0.27574843]\n"," [0.         0.6802825 ]\n"," [0.         0.26107113]\n"," [0.         0.91012606]\n"," [0.         0.99780135]\n"," [0.         0.28219947]\n"," [0.         0.67780878]\n"," [0.         0.0307359 ]\n"," [0.         0.56227906]\n"," [0.         0.411246  ]\n"," [0.         0.05795772]\n"," [0.         0.25090462]\n"," [0.         0.21181378]\n"," [0.         0.29220366]\n"," [0.         0.88893745]\n"," [0.         0.42623154]\n"," [0.         0.57998176]\n"," [0.         0.12494527]\n"," [0.         0.30780883]\n"," [0.         0.7055407 ]\n"," [0.         0.82566361]\n"," [0.         0.6806959 ]\n"," [0.         0.88137529]\n"," [0.         0.15283146]\n"," [0.         0.89075731]] 0.7964170532663036\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wdcnvh2aY58c","colab_type":"text"},"source":["입력된 값이 길지만, 1은 두번만 들어가 있기 때문에, 1이 찍인 원소를 찾습니다. `[1.    0.08361932]`과 `[1.         0.66439549]`이 확인이 됩니다. \n","\n","`0.08361932`와 `0.66439549`를 곱하면 `0.055556298045436`값이 나옵니다. `SimpleRNN` 레이어를 이용한 곱셈 문제 모델을 정의합니다. "]},{"cell_type":"code","metadata":{"id":"O4sS-sjlaCWv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1598347460351,"user_tz":-540,"elapsed":1471,"user":{"displayName":"ji-hoon Jung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEP4yfwhnQ1TZPZlHUU6YbHyd-BvHkAnUrwdBWex0=s64","userId":"03007390404369457996"}},"outputId":"e6345b4c-4538-463c-c98b-396abf5ca14e"},"source":["model = tf.keras.Sequential([\n","  tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]), \n","  tf.keras.layers.SimpleRNN(units=30), \n","  tf.keras.layers.Dense(1)\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","simple_rnn_1 (SimpleRNN)     (None, 100, 30)           990       \n","_________________________________________________________________\n","simple_rnn_2 (SimpleRNN)     (None, 30)                1830      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 31        \n","=================================================================\n","Total params: 2,851\n","Trainable params: 2,851\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7RnA6vqeaY7O","colab_type":"text"},"source":["`RNN` 레이어를 겹치기 위해 첫 번째 `SimpleRNN`레이어에서 `return_sequences=True`로 설정된 것을 확인할 수 있습니다. `return_sequences`는 레이어의 출력을 다음 레이어로 그대로 넘겨주게 됩니다. \n","\n","겹치는 레이어의 구조에 대한 이론 설명은 교재 `188페이지`를 참조하시기를 바랍니다. `RNN`은 `CNN`보다 학습 시간이 오래 걸리는 편이기 때문에 반드시 가속기를 `GPU`로 바꿔줍니다. "]},{"cell_type":"code","metadata":{"id":"5mZ_ynLObBMt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"5c80d6db-b77d-4e22-c0f1-c1e07dd3c425"},"source":["X = np.array(X)\n","Y = np.array(Y)\n","\n","# 2560개의 데이터만 학습시킵니다. 검증 데이터는 20%로 저장합니다. \n","history=model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","64/64 [==============================] - 8s 125ms/step - loss: 0.0594 - val_loss: 0.0476\n","Epoch 2/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0472 - val_loss: 0.0488\n","Epoch 3/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0461 - val_loss: 0.0474\n","Epoch 4/100\n","64/64 [==============================] - 8s 122ms/step - loss: 0.0471 - val_loss: 0.0470\n","Epoch 5/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0478 - val_loss: 0.0492\n","Epoch 6/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0467 - val_loss: 0.0465\n","Epoch 7/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0486 - val_loss: 0.0505\n","Epoch 8/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0462 - val_loss: 0.0471\n","Epoch 9/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0471 - val_loss: 0.0466\n","Epoch 10/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0459 - val_loss: 0.0476\n","Epoch 11/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0469 - val_loss: 0.0498\n","Epoch 12/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0465 - val_loss: 0.0478\n","Epoch 13/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0464 - val_loss: 0.0503\n","Epoch 14/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0467 - val_loss: 0.0467\n","Epoch 15/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0463 - val_loss: 0.0476\n","Epoch 16/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0453 - val_loss: 0.0474\n","Epoch 17/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0461 - val_loss: 0.0473\n","Epoch 18/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0453 - val_loss: 0.0476\n","Epoch 19/100\n","64/64 [==============================] - 7s 115ms/step - loss: 0.0457 - val_loss: 0.0461\n","Epoch 20/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0456 - val_loss: 0.0482\n","Epoch 21/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0452 - val_loss: 0.0466\n","Epoch 22/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0449 - val_loss: 0.0473\n","Epoch 23/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0448 - val_loss: 0.0466\n","Epoch 24/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0453 - val_loss: 0.0470\n","Epoch 25/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0453 - val_loss: 0.0464\n","Epoch 26/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0454 - val_loss: 0.0467\n","Epoch 27/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0450 - val_loss: 0.0489\n","Epoch 28/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0444 - val_loss: 0.0458\n","Epoch 29/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0440 - val_loss: 0.0476\n","Epoch 30/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0437 - val_loss: 0.0483\n","Epoch 31/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0435 - val_loss: 0.0514\n","Epoch 32/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0447 - val_loss: 0.0476\n","Epoch 33/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0435 - val_loss: 0.0470\n","Epoch 34/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0438 - val_loss: 0.0480\n","Epoch 35/100\n","64/64 [==============================] - 8s 122ms/step - loss: 0.0438 - val_loss: 0.0462\n","Epoch 36/100\n","64/64 [==============================] - 8s 122ms/step - loss: 0.0429 - val_loss: 0.0480\n","Epoch 37/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0450 - val_loss: 0.0487\n","Epoch 38/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0434 - val_loss: 0.0470\n","Epoch 39/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0429 - val_loss: 0.0487\n","Epoch 40/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0454 - val_loss: 0.0474\n","Epoch 41/100\n","64/64 [==============================] - 7s 115ms/step - loss: 0.0428 - val_loss: 0.0484\n","Epoch 42/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0416 - val_loss: 0.0483\n","Epoch 43/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0417 - val_loss: 0.0501\n","Epoch 44/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0413 - val_loss: 0.0497\n","Epoch 45/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0413 - val_loss: 0.0503\n","Epoch 46/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0408 - val_loss: 0.0482\n","Epoch 47/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0405 - val_loss: 0.0482\n","Epoch 48/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0408 - val_loss: 0.0486\n","Epoch 49/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0404 - val_loss: 0.0502\n","Epoch 50/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0400 - val_loss: 0.0486\n","Epoch 51/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0407 - val_loss: 0.0509\n","Epoch 52/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0396 - val_loss: 0.0497\n","Epoch 53/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0396 - val_loss: 0.0503\n","Epoch 54/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0390 - val_loss: 0.0515\n","Epoch 55/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0392 - val_loss: 0.0489\n","Epoch 56/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0391 - val_loss: 0.0513\n","Epoch 57/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0384 - val_loss: 0.0512\n","Epoch 58/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0383 - val_loss: 0.0526\n","Epoch 59/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0378 - val_loss: 0.0520\n","Epoch 60/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0377 - val_loss: 0.0526\n","Epoch 61/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0367 - val_loss: 0.0518\n","Epoch 62/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0372 - val_loss: 0.0506\n","Epoch 63/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0361 - val_loss: 0.0533\n","Epoch 64/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0356 - val_loss: 0.0519\n","Epoch 65/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0357 - val_loss: 0.0521\n","Epoch 66/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0356 - val_loss: 0.0562\n","Epoch 67/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0348 - val_loss: 0.0520\n","Epoch 68/100\n","64/64 [==============================] - 8s 117ms/step - loss: 0.0348 - val_loss: 0.0527\n","Epoch 69/100\n","64/64 [==============================] - 7s 116ms/step - loss: 0.0350 - val_loss: 0.0561\n","Epoch 70/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0334 - val_loss: 0.0544\n","Epoch 71/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0339 - val_loss: 0.0545\n","Epoch 72/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0340 - val_loss: 0.0586\n","Epoch 73/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0332 - val_loss: 0.0541\n","Epoch 74/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 75/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0319 - val_loss: 0.0544\n","Epoch 76/100\n","64/64 [==============================] - 8s 124ms/step - loss: 0.0321 - val_loss: 0.0584\n","Epoch 77/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0309 - val_loss: 0.0574\n","Epoch 78/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0313 - val_loss: 0.0573\n","Epoch 79/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0315 - val_loss: 0.0563\n","Epoch 80/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0308 - val_loss: 0.0582\n","Epoch 81/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0298 - val_loss: 0.0577\n","Epoch 82/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0295 - val_loss: 0.0586\n","Epoch 83/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0284 - val_loss: 0.0583\n","Epoch 84/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0288 - val_loss: 0.0589\n","Epoch 85/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0292 - val_loss: 0.0602\n","Epoch 86/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0287 - val_loss: 0.0608\n","Epoch 87/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0275 - val_loss: 0.0604\n","Epoch 88/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0278 - val_loss: 0.0595\n","Epoch 89/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0264 - val_loss: 0.0632\n","Epoch 90/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0273 - val_loss: 0.0605\n","Epoch 91/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0260 - val_loss: 0.0630\n","Epoch 92/100\n","64/64 [==============================] - 8s 121ms/step - loss: 0.0261 - val_loss: 0.0620\n","Epoch 93/100\n","64/64 [==============================] - 7s 117ms/step - loss: 0.0262 - val_loss: 0.0635\n","Epoch 94/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0246 - val_loss: 0.0599\n","Epoch 95/100\n","64/64 [==============================] - 8s 120ms/step - loss: 0.0247 - val_loss: 0.0610\n","Epoch 96/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0253 - val_loss: 0.0609\n","Epoch 97/100\n","64/64 [==============================] - 8s 118ms/step - loss: 0.0245 - val_loss: 0.0623\n","Epoch 98/100\n","64/64 [==============================] - 8s 119ms/step - loss: 0.0238 - val_loss: 0.0654\n","Epoch 99/100\n","42/64 [==================>...........] - ETA: 2s - loss: 0.0242"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9xEw56Lvbof2","colab_type":"text"},"source":["훈련 데이터의 손실(`loss`)과 검증 데이터의 손실(`var_loss`)는 감소하지 않고 오히려 증가하는 것 같습니다. 경향을 직관적으로 파악하기 위해 `history` 변수에 저장된 값으로 그래프를 그려봅니다. "]},{"cell_type":"code","metadata":{"id":"ZapF9f-yb-KJ","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLoCsg7hcHAf","colab_type":"text"},"source":["학습 결과는 전형적인 과적합 그래프를 보여줍니다. 테스트 데이터에 대한 예측은 어떨까요? 논문에서는 오차가 0.04 이상일 때 오답으로 처리합니다. "]},{"cell_type":"code","metadata":{"id":"yQh_p6-Mdvgy","colab_type":"code","colab":{}},"source":["model.evaluate(X[2560:], Y[2560:])\n","prediction=model.predict(X[2560:2560+5])\n","\n","# 5개 테스트 데이터에 대한 예측을 표시합니다. \n","for i in range(5): \n","  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n","\n","prediction = model.predict(X[2560:])\n","fail = 0\n","for i in range(len(prediction)):\n","  # 오차가 0.04 이상이면 오답입니다. \n","  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n","    fail +=1\n","\n","print('correctness:', (440-fail)/440*100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q27ljmgLe9Yx","colab_type":"text"},"source":["먼저 전체에 대한 평가는 `0.0667`의 `loss`가 나왔습니다. 위에서 본 100번째의 에포크의 `val_loss`인 `0.0664`보다도 높은 값으로, 네트워크가 학습 과정에서 한번도 못 본 테스트 데이터에 대해서는 잘 예측하지 못합니다. 5개의 테스트 데이터에 대한 샘플은 오차가 `0.01`에서 `0.18`까지 다양하게 나타나며, 가장 중요한 정확도는 `12.72`로 확인 됩니다. \n","\n","그렇다면 `LSTM`레이어는 어떨까요? 이 문제를 풀기 위해 시퀀셜 모델을 정의합니다.\n"]},{"cell_type":"code","metadata":{"id":"rOgxXjCLh48d","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","  tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,2]), \n","  tf.keras.layers.LSTM(units=30), \n","  tf.keras.layers.Dense(1)\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFfHHPbGi8mW","colab_type":"text"},"source":["차이점은 `SimpleRNN`을 `LSTM`으로 바꾼 것 뿐입니다. 네트워크의 학습코드도 동일합니다. "]},{"cell_type":"code","metadata":{"id":"b0ocxwtgjFXF","colab_type":"code","colab":{}},"source":["X = np.array(X)\n","Y = np.array(Y)\n","\n","# 2560개의 데이터만 학습시킵니다. 검증 데이터는 20%로 저장합니다. \n","history=model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)\n","\n","import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-1H4KDojOJT","colab_type":"text"},"source":["`loss`와 `val_loss`는 40에포크를 넘어가면서 매우 가파르게 줄어들어 0에 가까워집니다. `val_loss`는 변동폭이 `loss`보다 크지만 전체적으로는 계속 감소하는 경향을 보입니다. 학습이 매우 잘 된 것으로 보입니다.\n","\n","이번에는 실제로 테스트 데이터에 얼마나 정확하게 값을 예측하는지 확인해봅니다. "]},{"cell_type":"code","metadata":{"id":"OzYco1QpoTl6","colab_type":"code","colab":{}},"source":["model.evaluate(X[2560:], Y[2560:])\n","prediction=model.predict(X[2560:2560+5])\n","\n","# 5개 테스트 데이터에 대한 예측을 표시합니다. \n","for i in range(5): \n","  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n","\n","prediction = model.predict(X[2560:])\n","fail = 0\n","for i in range(len(prediction)):\n","  # 오차가 0.04 이상이면 오답입니다. \n","  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n","    fail +=1\n","\n","print('correctness:', (440-fail)/440*100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lIplmAzGoXTf","colab_type":"text"},"source":["테스트 데이터에 대한 `loss`는 0에 가까운 값이 나오고, 다섯 개의 샘플에 대한 오차도 0.04를 넘는 값이 없습니다. 또한 정확도 역시, 95.9%로 거의 96%에 가까운 것을 확인할 수 있습니다. \n","\n","곱셈문제를 푸는데 있어서 `LSTM`이 보다 적합하다는 것을 알 수 있습니다. \n","\n","다음 포스트에서는 `GRU`레이어와 `임베딩`레이어에 대해 학습하도록 합니다. \n","\n","## IV. 연습 파일\n","- [구글 Colab에서 직접 연습해보자](https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_1_2_RNN_theory(1).ipynb) \n","\n","## VI. Reference\n","\n","김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스."]}]}